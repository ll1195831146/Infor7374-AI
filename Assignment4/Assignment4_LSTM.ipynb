{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Assignment4.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ll1195831146/Infor7374-AI/blob/master/Assignment4/Assignment4_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "-rpFuvhvjGL0",
        "colab_type": "code",
        "outputId": "5d06c265-1ccd-4609-b712-7b5b9c0e9fe3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "import itertools\n",
        "import os\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import json\n",
        "import time\n",
        "import datetime\n",
        "import subprocess\n",
        "\n",
        "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "from keras.preprocessing import text, sequence\n",
        "\n",
        "from __future__ import absolute_import, division, print_function\n",
        "keras.__version__"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.2.4'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "metadata": {
        "id": "5IIGLg48nJvf",
        "colab_type": "code",
        "outputId": "40cd3202-9e70-470c-c355-bcb58a3323b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import re\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "EihwcokIm75h",
        "colab_type": "code",
        "outputId": "95982f08-e6f5-4ca6-d72d-094c2146dc87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 59
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "H-ckbIMfnDuj",
        "colab_type": "code",
        "outputId": "893ae323-1b49-4510-8932-798ec8743b0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mdrive\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PxdTqlYGoQ03",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def clean_str(text):\n",
        "    text = text.lower()\n",
        "    # Clean the text\n",
        "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
        "    text = re.sub(r\"what's\", \"what is \", text)\n",
        "    text = re.sub(r\"that's\", \"that is \", text)\n",
        "    text = re.sub(r\"there's\", \"there is \", text)\n",
        "    text = re.sub(r\"it's\", \"it is \", text)\n",
        "    text = re.sub(r\"\\'s\", \" \", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
        "    text = re.sub(r\"can't\", \"can not \", text)\n",
        "    text = re.sub(r\"n't\", \" not \", text)\n",
        "    text = re.sub(r\"i'm\", \"i am \", text)\n",
        "    text = re.sub(r\"\\'re\", \" are \", text)\n",
        "    text = re.sub(r\"\\'d\", \" would \", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
        "    text = re.sub(r\",\", \" \", text)\n",
        "    text = re.sub(r\"\\.\", \" \", text)\n",
        "    text = re.sub(r\"!\", \" ! \", text)\n",
        "    text = re.sub(r\"\\/\", \" \", text)\n",
        "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
        "    text = re.sub(r\"\\+\", \" + \", text)\n",
        "    text = re.sub(r\"\\-\", \" - \", text)\n",
        "    text = re.sub(r\"\\=\", \" = \", text)\n",
        "    text = re.sub(r\"'\", \" \", text)\n",
        "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
        "    text = re.sub(r\":\", \" : \", text)\n",
        "    text = re.sub(r\" e g \", \" eg \", text)\n",
        "    text = re.sub(r\" b g \", \" bg \", text)\n",
        "    text = re.sub(r\" u s \", \" american \", text)\n",
        "    text = re.sub(r\"\\0s\", \"0\", text)\n",
        "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
        "    text = re.sub(r\"e - mail\", \"email\", text)\n",
        "    text = re.sub(r\"j k\", \"jk\", text)\n",
        "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
        "\n",
        "    return text.strip()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0YmTo-cborq8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class2label = {'Other': 0,\n",
        "               'Message-Topic(e1,e2)': 1, 'Message-Topic(e2,e1)': 2,\n",
        "               'Product-Producer(e1,e2)': 3, 'Product-Producer(e2,e1)': 4,\n",
        "               'Instrument-Agency(e1,e2)': 5, 'Instrument-Agency(e2,e1)': 6,\n",
        "               'Entity-Destination(e1,e2)': 7, 'Entity-Destination(e2,e1)': 8,\n",
        "               'Cause-Effect(e1,e2)': 9, 'Cause-Effect(e2,e1)': 10,\n",
        "               'Component-Whole(e1,e2)': 11, 'Component-Whole(e2,e1)': 12,\n",
        "               'Entity-Origin(e1,e2)': 13, 'Entity-Origin(e2,e1)': 14,\n",
        "               'Member-Collection(e1,e2)': 15, 'Member-Collection(e2,e1)': 16,\n",
        "               'Content-Container(e1,e2)': 17, 'Content-Container(e2,e1)': 18}\n",
        "\n",
        "label2class = {0: 'Other',\n",
        "               1: 'Message-Topic(e1,e2)', 2: 'Message-Topic(e2,e1)',\n",
        "               3: 'Product-Producer(e1,e2)', 4: 'Product-Producer(e2,e1)',\n",
        "               5: 'Instrument-Agency(e1,e2)', 6: 'Instrument-Agency(e2,e1)',\n",
        "               7: 'Entity-Destination(e1,e2)', 8: 'Entity-Destination(e2,e1)',\n",
        "               9: 'Cause-Effect(e1,e2)', 10: 'Cause-Effect(e2,e1)',\n",
        "               11: 'Component-Whole(e1,e2)', 12: 'Component-Whole(e2,e1)',\n",
        "               13: 'Entity-Origin(e1,e2)', 14: 'Entity-Origin(e2,e1)',\n",
        "               15: 'Member-Collection(e1,e2)', 16: 'Member-Collection(e2,e1)',\n",
        "               17: 'Content-Container(e1,e2)', 18: 'Content-Container(e2,e1)'}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N1iUFZB6as4B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_word2vec(word2vec_path, embedding_dim, vocab):\n",
        "    # initial matrix with random uniform\n",
        "    initW = np.random.randn(len(vocab.vocabulary_), embedding_dim).astype(np.float32) * np.sqrt(2.0 / len(vocab.vocabulary_))\n",
        "    # load any vectors from the word2vec\n",
        "    print(\"Load word2vec file {0}\".format(word2vec_path))\n",
        "    with open(word2vec_path, \"rb\") as f:\n",
        "        header = f.readline()\n",
        "        vocab_size, layer1_size = map(int, header.split())\n",
        "        binary_len = np.dtype('float32').itemsize * layer1_size\n",
        "        for line in range(vocab_size):\n",
        "            word = []\n",
        "            while True:\n",
        "                ch = f.read(1).decode('latin-1')\n",
        "                if ch == ' ':\n",
        "                    word = ''.join(word)\n",
        "                    break\n",
        "                if ch != '\\n':\n",
        "                    word.append(ch)\n",
        "            idx = vocab.vocabulary_.get(word)\n",
        "            if idx != 0:\n",
        "                initW[idx] = np.fromstring(f.read(binary_len), dtype='float32')\n",
        "            else:\n",
        "                f.read(binary_len)\n",
        "    return initW\n",
        "\n",
        "\n",
        "def load_glove(word2vec_path, embedding_dim, vocab):\n",
        "    # initial matrix with random uniform\n",
        "    initW = np.random.randn(len(vocab.vocabulary_), embedding_dim).astype(np.float32) * np.sqrt(2.0 / len(vocab.vocabulary_))\n",
        "    # load any vectors from the word2vec\n",
        "    print(\"Load glove file {0}\".format(word2vec_path))\n",
        "    f = open(word2vec_path, 'r', encoding='utf8')\n",
        "    for line in f:\n",
        "        splitLine = line.split(' ')\n",
        "        word = splitLine[0]\n",
        "        embedding = np.asarray(splitLine[1:], dtype='float32')\n",
        "        idx = vocab.vocabulary_.get(word)\n",
        "        if idx != 0:\n",
        "            initW[idx] = embedding\n",
        "    return initW"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vl0qkdaUouPU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_data_and_labels(path):\n",
        "    data = []\n",
        "    lines = [line.strip() for line in open(path)]\n",
        "    max_sentence_length = 0\n",
        "    for idx in range(0, len(lines), 4):\n",
        "        id = lines[idx].split(\"\\t\")[0]\n",
        "        relation = lines[idx + 1]\n",
        "\n",
        "        sentence = lines[idx].split(\"\\t\")[1][1:-1]\n",
        "        sentence = sentence.replace('<e1>', ' _e11_ ')\n",
        "        sentence = sentence.replace('</e1>', ' _e12_ ')\n",
        "        sentence = sentence.replace('<e2>', ' _e21_ ')\n",
        "        sentence = sentence.replace('</e2>', ' _e22_ ')\n",
        "\n",
        "        sentence = clean_str(sentence)\n",
        "        tokens = nltk.word_tokenize(sentence)\n",
        "        if max_sentence_length < len(tokens):\n",
        "            max_sentence_length = len(tokens)\n",
        "        e1 = tokens.index(\"e12\") - 1\n",
        "        e2 = tokens.index(\"e22\") - 1\n",
        "        sentence = \" \".join(tokens)\n",
        "\n",
        "        data.append([id, sentence, e1, e2, relation])\n",
        "\n",
        "    print(path)\n",
        "    print(\"max sentence length = {}\\n\".format(max_sentence_length))\n",
        "\n",
        "    df = pd.DataFrame(data=data, columns=[\"id\", \"sentence\", \"e1\", \"e2\", \"relation\"])\n",
        "\n",
        "    pos1, pos2 = get_relative_position(df, 90)\n",
        "\n",
        "    df['label'] = [class2label[r] for r in df['relation']]\n",
        "\n",
        "    # Text Data\n",
        "    x_text = df['sentence'].tolist()\n",
        "    e1 = df['e1'].tolist()\n",
        "    e2 = df['e2'].tolist()\n",
        "\n",
        "    # Label Data\n",
        "    y = df['label']\n",
        "    labels_flat = y.values.ravel()\n",
        "    labels_count = np.unique(labels_flat).shape[0]\n",
        "\n",
        "    # convert class labels from scalars to one-hot vectors\n",
        "    # 0  => [1 0 0 0 0 ... 0 0 0 0 0]\n",
        "    # 1  => [0 1 0 0 0 ... 0 0 0 0 0]\n",
        "    # ...\n",
        "    # 18 => [0 0 0 0 0 ... 0 0 0 0 1]\n",
        "    def dense_to_one_hot(labels_dense, num_classes):\n",
        "        num_labels = labels_dense.shape[0]\n",
        "        index_offset = np.arange(num_labels) * num_classes\n",
        "        labels_one_hot = np.zeros((num_labels, num_classes))\n",
        "        labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
        "        return labels_one_hot\n",
        "\n",
        "    labels = dense_to_one_hot(labels_flat, labels_count)\n",
        "    labels = labels.astype(np.uint8)\n",
        "\n",
        "    return x_text, labels, e1, e2, pos1, pos2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vDLNSbr1tOCv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_relative_position(df, max_sentence_length):\n",
        "    # Position data\n",
        "    pos1 = []\n",
        "    pos2 = []\n",
        "    for df_idx in range(len(df)):\n",
        "        sentence = df.iloc[df_idx]['sentence']\n",
        "        tokens = nltk.word_tokenize(sentence)\n",
        "        e1 = df.iloc[df_idx]['e1']\n",
        "        e2 = df.iloc[df_idx]['e2']\n",
        "\n",
        "        p1 = \"\"\n",
        "        p2 = \"\"\n",
        "        for word_idx in range(len(tokens)):\n",
        "            p1 += str((max_sentence_length - 1) + word_idx - e1) + \" \"\n",
        "            p2 += str((max_sentence_length - 1) + word_idx - e2) + \" \"\n",
        "        pos1.append(p1)\n",
        "        pos2.append(p2)\n",
        "\n",
        "    return pos1, pos2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KDGyY3z-Zmvv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
        "    \"\"\"\n",
        "    Generates a batch iterator for a dataset.\n",
        "    \"\"\"\n",
        "    data = np.array(data)\n",
        "    data_size = len(data)\n",
        "    num_batches_per_epoch = int((len(data) - 1) / batch_size) + 1\n",
        "    for epoch in range(num_epochs):\n",
        "        # Shuffle the data at each epoch\n",
        "        if shuffle:\n",
        "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
        "            shuffled_data = data[shuffle_indices]\n",
        "        else:\n",
        "            shuffled_data = data\n",
        "        for batch_num in range(num_batches_per_epoch):\n",
        "            start_index = batch_num * batch_size\n",
        "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
        "            yield shuffled_data[start_index:end_index]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RgtcoR4XoyxR",
        "colab_type": "code",
        "outputId": "1a3b95b6-1479-4269-e0ce-5130c4272fef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "text_train, y_train, train_e1, train_e2, train_pos1, train_pos2 = load_data_and_labels(\"drive/My Drive/7374Assignment4/SemEval2010_task8_all_data/SemEval2010_task8_training/TRAIN_FILE.TXT\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive/My Drive/7374Assignment4/SemEval2010_task8_all_data/SemEval2010_task8_training/TRAIN_FILE.TXT\n",
            "max sentence length = 89\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cZFx8-ejpKsA",
        "colab_type": "code",
        "outputId": "f6fedfa6-90a0-4aa7-cc04-011b7bd93679",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "text_test, y_test, test_e1, test_e2, test_pos1, test_pos2 = load_data_and_labels(\"drive/My Drive/7374Assignment4/SemEval2010_task8_all_data/SemEval2010_task8_testing_keys/TEST_FILE_FULL.TXT\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive/My Drive/7374Assignment4/SemEval2010_task8_all_data/SemEval2010_task8_testing_keys/TEST_FILE_FULL.TXT\n",
            "max sentence length = 68\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yGrIs5LFsJX4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import sys\n",
        "\n",
        "def parse_args():\n",
        "    \"\"\"\n",
        "    Parse input arguments\n",
        "    \"\"\"\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    # Data loading params\n",
        "    parser.add_argument(\"--train_path\", default=\"drive/My Drive/7374Assignment4/SemEval2010_task8_all_data/SemEval2010_task8_training/TRAIN_FILE.TXT\",\n",
        "                        type=str, help=\"Path of train data\")\n",
        "    parser.add_argument(\"--test_path\", default=\"drive/My Drive/7374Assignment4/SemEval2010_task8_all_data/SemEval2010_task8_testing_keys/TEST_FILE_FULL.TXT\",\n",
        "                        type=str, help=\"Path of test data\")\n",
        "    parser.add_argument(\"--max_sentence_length\", default=90,\n",
        "                        type=int, help=\"Max sentence length in data\")\n",
        "\n",
        "    # Model Hyper-parameters\n",
        "    # Embeddings\n",
        "    parser.add_argument(\"--embeddings\", default=None,\n",
        "                        type=str, help=\"Embeddings {'word2vec', 'glove100', 'glove300', 'elmo'}\")\n",
        "    parser.add_argument(\"--embedding_size\", default=300,\n",
        "                        type=int, help=\"Dimensionality of word embedding (default: 300)\")\n",
        "    parser.add_argument(\"--pos_embedding_size\", default=50,\n",
        "                        type=int, help=\"Dimensionality of relative position embedding (default: 50)\")\n",
        "    parser.add_argument(\"--emb_dropout_keep_prob\", default=0.7,\n",
        "                        type=float, help=\"Dropout keep probability of embedding layer (default: 0.7)\")\n",
        "    # RNN\n",
        "    parser.add_argument(\"--hidden_size\", default=300,\n",
        "                        type=int, help=\"Dimensionality of RNN hidden (default: 300)\")\n",
        "    parser.add_argument(\"--rnn_dropout_keep_prob\", default=0.7,\n",
        "                        type=float, help=\"Dropout keep probability of RNN (default: 0.7)\")\n",
        "    # Attention\n",
        "    parser.add_argument(\"--num_heads\", default=4,\n",
        "                        type=int, help=\"Number of heads in multi-head attention (default: 4)\")\n",
        "    parser.add_argument(\"--attention_size\", default=50,\n",
        "                        type=int, help=\"Dimensionality of attention (default: 50)\")\n",
        "    # Misc\n",
        "    parser.add_argument(\"--desc\", default=\"\",\n",
        "                        type=str, help=\"Description for model\")\n",
        "    parser.add_argument(\"--dropout_keep_prob\", default=0.5,\n",
        "                        type=float, help=\"Dropout keep probability of output layer (default: 0.5)\")\n",
        "    parser.add_argument(\"--l2_reg_lambda\", default=1e-5,\n",
        "                        type=float, help=\"L2 regularization lambda (default: 1e-5)\")\n",
        "\n",
        "    # Training parameters\n",
        "    parser.add_argument(\"--batch_size\", default=20,\n",
        "                        type=int, help=\"Batch Size (default: 20)\")\n",
        "    parser.add_argument(\"--num_epochs\", default=100,\n",
        "                        type=int, help=\"Number of training epochs (Default: 100)\")\n",
        "    parser.add_argument(\"--display_every\", default=10,\n",
        "                        type=int, help=\"Number of iterations to display training information\")\n",
        "    parser.add_argument(\"--evaluate_every\", default=100,\n",
        "                        type=int, help=\"Evaluate model on dev set after this many steps (default: 100)\")\n",
        "    parser.add_argument(\"--num_checkpoints\", default=5,\n",
        "                        type=int, help=\"Number of checkpoints to store (default: 5)\")\n",
        "    parser.add_argument(\"--learning_rate\", default=1.0,\n",
        "                        type=float, help=\"Which learning rate to start with (Default: 1.0)\")\n",
        "    parser.add_argument(\"--decay_rate\", default=0.9,\n",
        "                        type=float, help=\"Decay rate for learning rate (Default: 0.9)\")\n",
        "\n",
        "    # Misc Parameters\n",
        "    parser.add_argument(\"--allow_soft_placement\", default=True,\n",
        "                        type=bool, help=\"Allow device soft device placement\")\n",
        "    parser.add_argument(\"--log_device_placement\", default=False,\n",
        "                        type=bool, help=\"Log placement of ops on devices\")\n",
        "    parser.add_argument(\"--gpu_allow_growth\", default=True,\n",
        "                        type=bool, help=\"Allow gpu memory growth\")\n",
        "\n",
        "    # Visualization Parameters\n",
        "    parser.add_argument(\"--checkpoint_dir\", default=None,\n",
        "                        type=str, help=\"Visualize this checkpoint\")\n",
        "\n",
        "    if len(sys.argv) == 0:\n",
        "        parser.print_help()\n",
        "        sys.exit(1)\n",
        "\n",
        "    print(\"\")\n",
        "    args = parser.parse_known_args()[0]\n",
        "    for arg in vars(args):\n",
        "        print(\"{}={}\".format(arg.upper(), getattr(args, arg)))\n",
        "    print(\"\")\n",
        "\n",
        "    return args"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JyQ-Wuvfw9Hz",
        "colab_type": "code",
        "outputId": "49808141-2baa-4f94-eca0-a28e6c8ea8a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 501
        }
      },
      "cell_type": "code",
      "source": [
        "FLAGS = parse_args()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "TRAIN_PATH=drive/My Drive/7374Assignment4/SemEval2010_task8_all_data/SemEval2010_task8_training/TRAIN_FILE.TXT\n",
            "TEST_PATH=drive/My Drive/7374Assignment4/SemEval2010_task8_all_data/SemEval2010_task8_testing_keys/TEST_FILE_FULL.TXT\n",
            "MAX_SENTENCE_LENGTH=90\n",
            "EMBEDDINGS=None\n",
            "EMBEDDING_SIZE=300\n",
            "POS_EMBEDDING_SIZE=50\n",
            "EMB_DROPOUT_KEEP_PROB=0.7\n",
            "HIDDEN_SIZE=300\n",
            "RNN_DROPOUT_KEEP_PROB=0.7\n",
            "NUM_HEADS=4\n",
            "ATTENTION_SIZE=50\n",
            "DESC=\n",
            "DROPOUT_KEEP_PROB=0.5\n",
            "L2_REG_LAMBDA=1e-05\n",
            "BATCH_SIZE=20\n",
            "NUM_EPOCHS=100\n",
            "DISPLAY_EVERY=10\n",
            "EVALUATE_EVERY=100\n",
            "NUM_CHECKPOINTS=5\n",
            "LEARNING_RATE=1.0\n",
            "DECAY_RATE=0.9\n",
            "ALLOW_SOFT_PLACEMENT=True\n",
            "LOG_DEVICE_PLACEMENT=False\n",
            "GPU_ALLOW_GROWTH=True\n",
            "CHECKPOINT_DIR=None\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gtWZHnf5w8F3",
        "colab_type": "code",
        "outputId": "7b0cdfe7-7f3d-4367-d627-31dd37ab6623",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431
        }
      },
      "cell_type": "code",
      "source": [
        "vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(90)\n",
        "vocab_processor.fit(text_train + text_test)\n",
        "x_train = np.array(list(vocab_processor.transform(text_train)))\n",
        "x_test = np.array(list(vocab_processor.transform(text_test)))\n",
        "text_train = np.array(text_train)\n",
        "text_test = np.array(text_test)\n",
        "print(\"\\nText Vocabulary Size: {:d}\".format(len(vocab_processor.vocabulary_)))\n",
        "print(\"x_train = {0}\".format(x_train.shape))\n",
        "print(\"y_train = {0}\".format(y_train.shape))\n",
        "print(\"x_test = {0}\".format(x_test.shape))\n",
        "print(\"y_test = {0}\".format(y_test.shape))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-13-eb4cf98214cd>:1: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tensorflow/transform or tf.data.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tensorflow/transform or tf.data.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:170: tokenizer (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tensorflow/transform or tf.data.\n",
            "\n",
            "Text Vocabulary Size: 22384\n",
            "x_train = (8000, 90)\n",
            "y_train = (8000, 19)\n",
            "x_test = (2717, 90)\n",
            "y_test = (2717, 19)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mMs23rCEyH8x",
        "colab_type": "code",
        "outputId": "480caad7-a84c-45f1-dc6a-258aea5816f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "pos_vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(FLAGS.max_sentence_length)\n",
        "pos_vocab_processor.fit(train_pos1 + train_pos2 + test_pos1 + test_pos2)\n",
        "train_p1 = np.array(list(pos_vocab_processor.transform(train_pos1)))\n",
        "train_p2 = np.array(list(pos_vocab_processor.transform(train_pos2)))\n",
        "test_p1 = np.array(list(pos_vocab_processor.transform(test_pos1)))\n",
        "test_p2 = np.array(list(pos_vocab_processor.transform(test_pos2)))\n",
        "print(\"\\nPosition Vocabulary Size: {:d}\".format(len(pos_vocab_processor.vocabulary_)))\n",
        "print(\"train_p1 = {0}\".format(train_p1.shape))\n",
        "print(\"test_p1 = {0}\".format(test_p1.shape))\n",
        "print(\"\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Position Vocabulary Size: 162\n",
            "train_p1 = (8000, 90)\n",
            "test_p1 = (2717, 90)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oinb5Ifu2lAN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def initializer():\n",
        "    return tf.keras.initializers.glorot_normal()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u2x6tEqa2lDW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_word2vec(word2vec_path, embedding_dim, vocab):\n",
        "    # initial matrix with random uniform\n",
        "    initW = np.random.randn(len(vocab.vocabulary_), embedding_dim).astype(np.float32) * np.sqrt(2.0 / len(vocab.vocabulary_))\n",
        "    # load any vectors from the word2vec\n",
        "    print(\"Load word2vec file {0}\".format(word2vec_path))\n",
        "    with open(word2vec_path, \"rb\") as f:\n",
        "        header = f.readline()\n",
        "        vocab_size, layer1_size = map(int, header.split())\n",
        "        binary_len = np.dtype('float32').itemsize * layer1_size\n",
        "        for line in range(vocab_size):\n",
        "            word = []\n",
        "            while True:\n",
        "                ch = f.read(1).decode('latin-1')\n",
        "                if ch == ' ':\n",
        "                    word = ''.join(word)\n",
        "                    break\n",
        "                if ch != '\\n':\n",
        "                    word.append(ch)\n",
        "            idx = vocab.vocabulary_.get(word)\n",
        "            if idx != 0:\n",
        "                initW[idx] = np.fromstring(f.read(binary_len), dtype='float32')\n",
        "            else:\n",
        "                f.read(binary_len)\n",
        "    return initW"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PRRVp2-d2lGa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_glove(word2vec_path, embedding_dim, vocab):\n",
        "    # initial matrix with random uniform\n",
        "    initW = np.random.randn(len(vocab.vocabulary_), embedding_dim).astype(np.float32) * np.sqrt(2.0 / len(vocab.vocabulary_))\n",
        "    # load any vectors from the word2vec\n",
        "    print(\"Load glove file {0}\".format(word2vec_path))\n",
        "    f = open(word2vec_path, 'r', encoding='utf8')\n",
        "    for line in f:\n",
        "        splitLine = line.split(' ')\n",
        "        word = splitLine[0]\n",
        "        embedding = np.asarray(splitLine[1:], dtype='float32')\n",
        "        idx = vocab.vocabulary_.get(word)\n",
        "        if idx != 0:\n",
        "            initW[idx] = embedding\n",
        "    return initW"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fAGRdcaH2lJk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def attention(inputs, e1, e2, p1, p2, attention_size):\n",
        "    # inputs = (batch, seq_len, hidden)\n",
        "    # e1, e2 = (batch, seq_len)\n",
        "    # p1, p2 = (batch, seq_len, dist_emb_size)\n",
        "    # attention_size = scalar(int)\n",
        "    def extract_entity(x, e):\n",
        "        e_idx = tf.concat([tf.expand_dims(tf.range(tf.shape(e)[0]), axis=-1), tf.expand_dims(e, axis=-1)], axis=-1)\n",
        "        return tf.gather_nd(x, e_idx)  # (batch, hidden)\n",
        "    seq_len = tf.shape(inputs)[1]  # fixed at run-time\n",
        "    hidden_size = inputs.shape[2].value  # fixed at compile-time\n",
        "    latent_size = hidden_size\n",
        "\n",
        "    # Latent Relation Variable based on Entities\n",
        "    e1_h = extract_entity(inputs, e1)  # (batch, hidden)\n",
        "    e2_h = extract_entity(inputs, e2)  # (batch, hidden)\n",
        "    e1_type, e2_type, e1_alphas, e2_alphas = latent_type_attention(e1_h, e2_h,\n",
        "                                                                   num_type=3,\n",
        "                                                                   latent_size=latent_size)  # (batch, hidden)\n",
        "    e1_h = tf.concat([e1_h, e1_type], axis=-1)  # (batch, hidden+latent)\n",
        "    e2_h = tf.concat([e2_h, e2_type], axis=-1)  # (batch, hidden+latent)\n",
        "\n",
        "    # v*tanh(W*[h;p1;p2]+W*[e1;e2]) 85.18%? 84.83% 84.55%\n",
        "    e_h = tf.layers.dense(tf.concat([e1_h, e2_h], -1), attention_size, use_bias=False, kernel_initializer=initializer())\n",
        "    e_h = tf.reshape(tf.tile(e_h, [1, seq_len]), [-1, seq_len, attention_size])\n",
        "    v = tf.layers.dense(tf.concat([inputs, p1, p2], axis=-1), attention_size, use_bias=False, kernel_initializer=initializer())\n",
        "    v = tf.tanh(tf.add(v, e_h))\n",
        "\n",
        "    u_omega = tf.get_variable(\"u_omega\", [attention_size], initializer=initializer())\n",
        "    vu = tf.tensordot(v, u_omega, axes=1, name='vu')  # (batch, seq_len)\n",
        "    alphas = tf.nn.softmax(vu, name='alphas')  # (batch, seq_len)\n",
        "\n",
        "    # v*tanh(W*[h;p1;p2;e1;e2]) 85.18% 84.41%\n",
        "    # e1_h = tf.reshape(tf.tile(e1_h, [1, seq_len]), [-1, seq_len, hidden_size+latent_size])\n",
        "    # e2_h = tf.reshape(tf.tile(e2_h, [1, seq_len]), [-1, seq_len, hidden_size+latent_size])\n",
        "    # v = tf.concat([inputs, p1, p2, e1_h, e2_h], axis=-1)\n",
        "    # v = tf.layers.dense(v, attention_size, activation=tf.tanh, kernel_initializer=initializer())\n",
        "    #\n",
        "    # u_omega = tf.get_variable(\"u_omega\", [attention_size], initializer=initializer())\n",
        "    # vu = tf.tensordot(v, u_omega, axes=1, name='vu')  # (batch, seq_len)\n",
        "    # alphas = tf.nn.softmax(vu, name='alphas')  # (batch, seq_len)\n",
        "\n",
        "    # output\n",
        "    output = tf.reduce_sum(inputs * tf.expand_dims(alphas, -1), 1)  # (batch, hidden)\n",
        "\n",
        "    return output, alphas, e1_alphas, e2_alphas"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rZ_kkfDKXGP2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def latent_type_attention(e1, e2, num_type, latent_size):\n",
        "    # Latent Entity Type Vectors\n",
        "    latent_type = tf.get_variable(\"latent_type\", shape=[num_type, latent_size], initializer=initializer())\n",
        "\n",
        "    # e1_h = tf.layers.dense(e1, latent_size, kernel_initializer=initializer())\n",
        "    # e2_h = tf.layers.dense(e2, latent_size, kernel_initializer=initializer())\n",
        "\n",
        "    e1_sim = tf.matmul(e1, tf.transpose(latent_type))  # (batch, num_type)\n",
        "    e1_alphas = tf.nn.softmax(e1_sim, name='e1_alphas')  # (batch, num_type)\n",
        "    e1_type = tf.matmul(e1_alphas, latent_type, name='e1_type')  # (batch, hidden)\n",
        "\n",
        "    e2_sim = tf.matmul(e2, tf.transpose(latent_type))  # (batch, num_type)\n",
        "    e2_alphas = tf.nn.softmax(e2_sim, name='e2_alphas')  # (batch, num_type)\n",
        "    e2_type = tf.matmul(e2_alphas, latent_type, name='e2_type')  # (batch, hidden)\n",
        "\n",
        "    return e1_type, e2_type, e1_alphas, e2_alphas"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MXvp-3Eo2lMb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def multihead_attention(queries, keys, num_units, num_heads,\n",
        "                        dropout_rate=0, scope=\"multihead_attention\", reuse=None):\n",
        "    with tf.variable_scope(scope, reuse=reuse):\n",
        "        # Linear projections\n",
        "        Q = tf.layers.dense(queries, num_units, kernel_initializer=initializer())  # (N, T_q, C)\n",
        "        K = tf.layers.dense(keys, num_units, kernel_initializer=initializer())  # (N, T_k, C)\n",
        "        V = tf.layers.dense(keys, num_units, kernel_initializer=initializer())  # (N, T_k, C)\n",
        "\n",
        "        # Split and concat\n",
        "        Q_ = tf.concat(tf.split(Q, num_heads, axis=2), axis=0)  # (h*N, T_q, C/h)\n",
        "        K_ = tf.concat(tf.split(K, num_heads, axis=2), axis=0)  # (h*N, T_k, C/h)\n",
        "        V_ = tf.concat(tf.split(V, num_heads, axis=2), axis=0)  # (h*N, T_k, C/h)\n",
        "\n",
        "        # Multiplication\n",
        "        outputs = tf.matmul(Q_, tf.transpose(K_, [0, 2, 1]))  # (h*N, T_q, T_k)\n",
        "\n",
        "        # Scale\n",
        "        outputs /= K_.get_shape().as_list()[-1] ** 0.5\n",
        "\n",
        "        # Key Masking\n",
        "        key_masks = tf.sign(tf.abs(tf.reduce_sum(keys, axis=-1)))  # (N, T_k)\n",
        "        key_masks = tf.tile(key_masks, [num_heads, 1])  # (h*N, T_k)\n",
        "        key_masks = tf.tile(tf.expand_dims(key_masks, 1), [1, tf.shape(queries)[1], 1])  # (h*N, T_q, T_k)\n",
        "\n",
        "        paddings = tf.ones_like(outputs) * (-2 ** 32 + 1)\n",
        "        outputs = tf.where(tf.equal(key_masks, 0), paddings, outputs)  # (h*N, T_q, T_k)\n",
        "\n",
        "        # Activation\n",
        "        alphas = tf.nn.softmax(outputs)  # (h*N, T_q, T_k)\n",
        "\n",
        "        # Query Masking\n",
        "        query_masks = tf.sign(tf.abs(tf.reduce_sum(queries, axis=-1)))  # (N, T_q)\n",
        "        query_masks = tf.tile(query_masks, [num_heads, 1])  # (h*N, T_q)\n",
        "        query_masks = tf.tile(tf.expand_dims(query_masks, -1), [1, 1, tf.shape(keys)[1]])  # (h*N, T_q, T_k)\n",
        "        alphas *= query_masks  # broadcasting. (N, T_q, C)\n",
        "\n",
        "        # Dropouts\n",
        "        alphas = tf.layers.dropout(alphas, rate=dropout_rate, training=tf.convert_to_tensor(True))\n",
        "\n",
        "        # Weighted sum\n",
        "        outputs = tf.matmul(alphas, V_)  # ( h*N, T_q, C/h)\n",
        "        # Restore shape\n",
        "        outputs = tf.concat(tf.split(outputs, num_heads, axis=0), axis=2)  # (N, T_q, C)\n",
        "\n",
        "        # Linear\n",
        "        outputs = tf.layers.dense(outputs, num_units, activation=tf.nn.relu, kernel_initializer=initializer())\n",
        "\n",
        "        # Residual connection\n",
        "        outputs += queries\n",
        "        # Normalize\n",
        "        outputs = layer_norm(outputs)  # (N, T_q, C)\n",
        "\n",
        "    return outputs, alphas"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f9PKygyn_CRN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def layer_norm(inputs, epsilon=1e-8, scope=\"layer_norm\", reuse=None):\n",
        "    with tf.variable_scope(scope, reuse=reuse):\n",
        "        inputs_shape = inputs.get_shape()\n",
        "        params_shape = inputs_shape[-1:]\n",
        "\n",
        "        mean, variance = tf.nn.moments(inputs, [-1], keep_dims=True)\n",
        "        beta = tf.Variable(tf.zeros(params_shape))\n",
        "        gamma = tf.Variable(tf.ones(params_shape))\n",
        "        normalized = (inputs - mean) / ((variance + epsilon) ** (.5))\n",
        "        outputs = gamma * normalized + beta\n",
        "    return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Vt4U-Ckd3gWX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Logger:\n",
        "    def __init__(self, out_dir):\n",
        "        self.log_dir = os.path.abspath(os.path.join(out_dir, \"logs\"))\n",
        "        os.makedirs(self.log_dir)\n",
        "        self.log_path = os.path.abspath(os.path.join(self.log_dir, \"logs.txt\"))\n",
        "        self.log_file = open(self.log_path, \"w\")\n",
        "\n",
        "        self.print_hyperparameters()\n",
        "\n",
        "        self.best_f1 = 0.0\n",
        "\n",
        "    def print_hyperparameters(self):\n",
        "        self.log_file.write(\"\\n================ Hyper-parameters ================\\n\\n\")\n",
        "        for arg in vars(FLAGS):\n",
        "            self.log_file.write(\"{}={}\\n\".format(arg.upper(), getattr(FLAGS, arg)))\n",
        "        self.log_file.write(\"\\n==================================================\\n\\n\")\n",
        "\n",
        "    def logging_train(self, step, loss, accuracy):\n",
        "        time_str = datetime.datetime.now().isoformat()\n",
        "        log = \"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy)\n",
        "        self.log_file.write(log+\"\\n\")\n",
        "        print(log)\n",
        "\n",
        "    def logging_eval(self, step, loss, accuracy, predictions):\n",
        "        self.log_file.write(\"\\nEvaluation:\\n\")\n",
        "        # loss & acc\n",
        "        time_str = datetime.datetime.now().isoformat()\n",
        "        log = \"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy)\n",
        "        self.log_file.write(log + \"\\n\")\n",
        "        print(log)\n",
        "\n",
        "        # f1-score\n",
        "        prediction_path = os.path.abspath(os.path.join(self.log_dir, \"predictions.txt\"))\n",
        "        prediction_file = open(prediction_path, 'w')\n",
        "        for i in range(len(predictions)):\n",
        "            prediction_file.write(\"{}\\t{}\\n\".format(i, label2class[predictions[i]]))\n",
        "#         prediction_file.close()\n",
        "        print(\"===========logger===========\")\n",
        "        perl_path = \"drive/My Drive/7374Assignment4/SemEval2010_task8_all_data/SemEval2010_task8_scorer-v1.2/semeval2010_task8_scorer-v1.2.pl\"\n",
        "#         os.path.join(os.path.curdir,\n",
        "#                                  \"SemEval2010_task8_all_data\",\n",
        "#                                  \"SemEval2010_task8_scorer-v1.2\",\n",
        "#                                  \"semeval2010_task8_scorer-v1.2.pl\")\n",
        "        print(prediction_path)\n",
        "        target_path = \"drive/My Drive/7374Assignment4/SemEval2010_task8_all_data/resource/target.txt\"\n",
        "        process = subprocess.Popen([\"perl\", perl_path, prediction_path, target_path], stdout=subprocess.PIPE)\n",
        "        print(process)\n",
        "        print(str(process.communicate()[0]))\n",
        "        str_parse = str(process.communicate()[0]).split(\"\\\\n\")[-2]\n",
        "        idx = str_parse.find('%')\n",
        "        f1_score = float(str_parse[idx-5:idx])\n",
        "\n",
        "        self.best_f1 = max(self.best_f1, f1_score)\n",
        "        f1_log = \"<<< (9+1)-WAY EVALUATION TAKING DIRECTIONALITY INTO ACCOUNT -- OFFICIAL >>>:\\n\" \\\n",
        "                 \"macro-averaged F1-score = {:g}%, Best = {:g}%\\n\".format(f1_score, self.best_f1)\n",
        "        self.log_file.write(f1_log + \"\\n\")\n",
        "        print(f1_log)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YpoaA2H-2lPB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class EntityAttentionLSTM:\n",
        "    def __init__(self, sequence_length, num_classes,\n",
        "                 vocab_size, embedding_size, pos_vocab_size, pos_embedding_size,\n",
        "                 hidden_size, num_heads, attention_size,\n",
        "                 use_elmo=False, l2_reg_lambda=0.0):\n",
        "        # Placeholders for input, output and dropout\n",
        "        self.input_x = tf.placeholder(tf.int32, shape=[None, sequence_length], name='input_x')\n",
        "        self.input_y = tf.placeholder(tf.float32, shape=[None, num_classes], name='input_y')\n",
        "        self.input_text = tf.placeholder(tf.string, shape=[None, ], name='input_text')\n",
        "        self.input_e1 = tf.placeholder(tf.int32, shape=[None, ], name='input_e1')\n",
        "        self.input_e2 = tf.placeholder(tf.int32, shape=[None, ], name='input_e2')\n",
        "        self.input_p1 = tf.placeholder(tf.int32, shape=[None, sequence_length], name='input_p1')\n",
        "        self.input_p2 = tf.placeholder(tf.int32, shape=[None, sequence_length], name='input_p2')\n",
        "        self.emb_dropout_keep_prob = tf.placeholder(tf.float32, name='emb_dropout_keep_prob')\n",
        "        self.rnn_dropout_keep_prob = tf.placeholder(tf.float32, name='rnn_dropout_keep_prob')\n",
        "        self.dropout_keep_prob = tf.placeholder(tf.float32, name='dropout_keep_prob')\n",
        "\n",
        "        if use_elmo:\n",
        "            # Contextual Embedding Layer\n",
        "            with tf.variable_scope(\"elmo-embeddings\"):\n",
        "                elmo_model = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=True)\n",
        "                self.embedded_chars = elmo_model(self.input_text, signature=\"default\", as_dict=True)[\"elmo\"]\n",
        "        else:\n",
        "            # Word Embedding Layer\n",
        "            with tf.device('/cpu:0'), tf.variable_scope(\"word-embeddings\"):\n",
        "                self.W_text = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -0.25, 0.25), name=\"W_text\")\n",
        "                self.embedded_chars = tf.nn.embedding_lookup(self.W_text, self.input_x)\n",
        "\n",
        "        # Position Embedding Layer\n",
        "        with tf.device('/cpu:0'), tf.variable_scope(\"position-embeddings\"):\n",
        "            self.W_pos = tf.get_variable(\"W_pos\", [pos_vocab_size, pos_embedding_size], initializer=initializer())\n",
        "            self.p1 = tf.nn.embedding_lookup(self.W_pos, self.input_p1)[:, :tf.shape(self.embedded_chars)[1]]\n",
        "            self.p2 = tf.nn.embedding_lookup(self.W_pos, self.input_p2)[:, :tf.shape(self.embedded_chars)[1]]\n",
        "\n",
        "        # Dropout for Word Embedding\n",
        "        with tf.variable_scope('dropout-embeddings'):\n",
        "            self.embedded_chars = tf.nn.dropout(self.embedded_chars,  self.emb_dropout_keep_prob)\n",
        "\n",
        "        # Self Attention\n",
        "        with tf.variable_scope(\"self-attention\"):\n",
        "            self.self_attn, self.self_alphas = multihead_attention(self.embedded_chars, self.embedded_chars,\n",
        "                                                                   num_units=embedding_size, num_heads=num_heads)\n",
        "\n",
        "        # Bidirectional LSTM\n",
        "        with tf.variable_scope(\"bi-lstm\"):\n",
        "            _fw_cell = tf.nn.rnn_cell.LSTMCell(hidden_size, initializer=initializer())\n",
        "            fw_cell = tf.nn.rnn_cell.DropoutWrapper(_fw_cell, self.rnn_dropout_keep_prob)\n",
        "            _bw_cell = tf.nn.rnn_cell.LSTMCell(hidden_size, initializer=initializer())\n",
        "            bw_cell = tf.nn.rnn_cell.DropoutWrapper(_bw_cell, self.rnn_dropout_keep_prob)\n",
        "            self.rnn_outputs, _ = tf.nn.bidirectional_dynamic_rnn(cell_fw=fw_cell,\n",
        "                                                                  cell_bw=bw_cell,\n",
        "                                                                  inputs=self.self_attn,\n",
        "                                                                  sequence_length=self._length(self.input_x),\n",
        "                                                                  dtype=tf.float32)\n",
        "            self.rnn_outputs = tf.concat(self.rnn_outputs, axis=-1)\n",
        "\n",
        "        # Attention\n",
        "        with tf.variable_scope('attention'):\n",
        "            self.attn, self.alphas, self.e1_alphas, self.e2_alphas = attention(self.rnn_outputs,\n",
        "                                                                               self.input_e1, self.input_e2,\n",
        "                                                                               self.p1, self.p2,\n",
        "                                                                               attention_size=attention_size)\n",
        "\n",
        "        # Dropout\n",
        "        with tf.variable_scope('dropout'):\n",
        "            self.h_drop = tf.nn.dropout(self.attn, self.dropout_keep_prob)\n",
        "\n",
        "        # Fully connected layer\n",
        "        with tf.variable_scope('output'):\n",
        "            self.logits = tf.layers.dense(self.h_drop, num_classes, kernel_initializer=initializer())\n",
        "            self.predictions = tf.argmax(self.logits, 1, name=\"predictions\")\n",
        "\n",
        "        # Calculate mean cross-entropy loss\n",
        "        with tf.variable_scope(\"loss\"):\n",
        "            losses = tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.logits, labels=self.input_y)\n",
        "            self.l2 = tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables()])\n",
        "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * self.l2\n",
        "\n",
        "        # Accuracy\n",
        "        with tf.variable_scope(\"accuracy\"):\n",
        "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
        "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32), name=\"accuracy\")\n",
        "\n",
        "    # Length of the sequence data\n",
        "    @staticmethod\n",
        "    def _length(seq):\n",
        "        relevant = tf.sign(tf.abs(seq))\n",
        "        length = tf.reduce_sum(relevant, reduction_indices=1)\n",
        "        length = tf.cast(length, tf.int32)\n",
        "        return length"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ajP__exzPERy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        },
        "outputId": "50a8048f-bbec-4e6f-b433-68d725bad3cb"
      },
      "cell_type": "code",
      "source": [
        "with tf.Graph().as_default():\n",
        "  session_conf = tf.ConfigProto(\n",
        "      allow_soft_placement=FLAGS.allow_soft_placement,\n",
        "      log_device_placement=FLAGS.log_device_placement)\n",
        "  session_conf.gpu_options.allow_growth = FLAGS.gpu_allow_growth\n",
        "  sess = tf.Session(config=session_conf)\n",
        "  with sess.as_default():\n",
        "    model = EntityAttentionLSTM(\n",
        "        sequence_length=x_train.shape[1],\n",
        "        num_classes=y_train.shape[1],\n",
        "        vocab_size=len(vocab_processor.vocabulary_),\n",
        "        embedding_size=FLAGS.embedding_size,\n",
        "        pos_vocab_size=len(pos_vocab_processor.vocabulary_),\n",
        "        pos_embedding_size=FLAGS.pos_embedding_size,\n",
        "        hidden_size=FLAGS.hidden_size,\n",
        "        num_heads=FLAGS.num_heads,\n",
        "        attention_size=FLAGS.attention_size,\n",
        "        use_elmo=(FLAGS.embeddings == 'elmo'),\n",
        "        l2_reg_lambda=FLAGS.l2_reg_lambda)\n",
        "\n",
        "    # Define Training procedure\n",
        "    global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
        "    optimizer = tf.train.AdadeltaOptimizer(FLAGS.learning_rate, FLAGS.decay_rate, 1e-6)\n",
        "    gvs = optimizer.compute_gradients(model.loss)\n",
        "    capped_gvs = [(tf.clip_by_value(grad, -1.0, 1.0), var) for grad, var in gvs]\n",
        "    train_op = optimizer.apply_gradients(capped_gvs, global_step=global_step)\n",
        "\n",
        "    # Output directory for models and summaries\n",
        "    timestamp = str(int(time.time()))\n",
        "    out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
        "    print(\"\\nWriting to {}\\n\".format(out_dir))\n",
        "\n",
        "    # Logger\n",
        "    logger = Logger(out_dir)\n",
        "\n",
        "    # Summaries for loss and accuracy\n",
        "    loss_summary = tf.summary.scalar(\"loss\", model.loss)\n",
        "    acc_summary = tf.summary.scalar(\"accuracy\", model.accuracy)\n",
        "\n",
        "    # Train Summaries\n",
        "    train_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
        "    train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
        "    train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
        "\n",
        "    # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
        "    checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
        "    checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
        "    if not os.path.exists(checkpoint_dir):\n",
        "        os.makedirs(checkpoint_dir)\n",
        "    saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n",
        "\n",
        "    # Write vocabulary\n",
        "    vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
        "    pos_vocab_processor.save(os.path.join(out_dir, \"pos_vocab\"))\n",
        "\n",
        "    # Initialize all variables\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    if FLAGS.embeddings == \"word2vec\":\n",
        "        pretrain_W = load_word2vec('resource/GoogleNews-vectors-negative300.bin', FLAGS.embedding_size, vocab_processor)\n",
        "        sess.run(model.W_text.assign(pretrain_W))\n",
        "        print(\"Success to load pre-trained word2vec model!\\n\")\n",
        "    elif FLAGS.embeddings == \"glove100\":\n",
        "        pretrain_W = load_glove('resource/glove.6B.100d.txt', FLAGS.embedding_size, vocab_processor)\n",
        "        sess.run(model.W_text.assign(pretrain_W))\n",
        "        print(\"Success to load pre-trained glove100 model!\\n\")\n",
        "    elif FLAGS.embeddings == \"glove300\":\n",
        "        pretrain_W = load_glove('resource/glove.840B.300d.txt', FLAGS.embedding_size, vocab_processor)\n",
        "        sess.run(model.W_text.assign(pretrain_W))\n",
        "        print(\"Success to load pre-trained glove300 model!\\n\")\n",
        "\n",
        "    # Generate batches\n",
        "    train_batches = batch_iter(list(zip(x_train, y_train, text_train,\n",
        "                                                     train_e1, train_e2, train_p1, train_p2)),\n",
        "                                            FLAGS.batch_size, FLAGS.num_epochs)\n",
        "    # Training loop. For each batch...\n",
        "    best_f1 = 0.0  # For save checkpoint(model)\n",
        "    for train_batch in train_batches:\n",
        "        train_bx, train_by, train_btxt, train_be1, train_be2, train_bp1, train_bp2 = zip(*train_batch)\n",
        "        feed_dict = {\n",
        "            model.input_x: train_bx,\n",
        "            model.input_y: train_by,\n",
        "            model.input_text: train_btxt,\n",
        "            model.input_e1: train_be1,\n",
        "            model.input_e2: train_be2,\n",
        "            model.input_p1: train_bp1,\n",
        "            model.input_p2: train_bp2,\n",
        "            model.emb_dropout_keep_prob: FLAGS.emb_dropout_keep_prob,\n",
        "            model.rnn_dropout_keep_prob: FLAGS.rnn_dropout_keep_prob,\n",
        "            model.dropout_keep_prob: FLAGS.dropout_keep_prob\n",
        "        }\n",
        "        _, step, summaries, loss, accuracy = sess.run(\n",
        "            [train_op, global_step, train_summary_op, model.loss, model.accuracy], feed_dict)\n",
        "        train_summary_writer.add_summary(summaries, step)\n",
        "\n",
        "        # Training log display\n",
        "        if step % FLAGS.display_every == 0:\n",
        "            logger.logging_train(step, loss, accuracy)\n",
        "\n",
        "        # Evaluation\n",
        "        if step % FLAGS.evaluate_every == 0:\n",
        "            print(\"\\nEvaluation:\")\n",
        "            # Generate batches\n",
        "            test_batches = batch_iter(list(zip(x_test, y_test, text_test,\n",
        "                                                            test_e1, test_e2, test_p1, test_p2)),\n",
        "                                                   FLAGS.batch_size, 1, shuffle=False)\n",
        "            # Training loop. For each batch...\n",
        "            losses = 0.0\n",
        "            accuracy = 0.0\n",
        "            predictions = []\n",
        "            iter_cnt = 0\n",
        "            for test_batch in test_batches:\n",
        "                test_bx, test_by, test_btxt, test_be1, test_be2, test_bp1, test_bp2 = zip(*test_batch)\n",
        "                feed_dict = {\n",
        "                    model.input_x: test_bx,\n",
        "                    model.input_y: test_by,\n",
        "                    model.input_text: test_btxt,\n",
        "                    model.input_e1: test_be1,\n",
        "                    model.input_e2: test_be2,\n",
        "                    model.input_p1: test_bp1,\n",
        "                    model.input_p2: test_bp2,\n",
        "                    model.emb_dropout_keep_prob: 1.0,\n",
        "                    model.rnn_dropout_keep_prob: 1.0,\n",
        "                    model.dropout_keep_prob: 1.0\n",
        "                }\n",
        "                loss, acc, pred = sess.run(\n",
        "                    [model.loss, model.accuracy, model.predictions], feed_dict)\n",
        "                losses += loss\n",
        "                accuracy += acc\n",
        "                predictions += pred.tolist()\n",
        "                iter_cnt += 1\n",
        "            losses /= iter_cnt\n",
        "            accuracy /= iter_cnt\n",
        "            predictions = np.array(predictions, dtype='int')\n",
        "\n",
        "            logger.logging_eval(step, loss, accuracy, predictions)\n",
        "\n",
        "            # Model checkpoint\n",
        "            if best_f1 < logger.best_f1:\n",
        "                best_f1 = logger.best_f1\n",
        "                path = saver.save(sess, checkpoint_prefix+\"-{:.3g}\".format(best_f1), global_step=step)\n",
        "                print(\"Saved model checkpoint to {}\\n\".format(path))"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Writing to /content/runs/1554166323\n",
            "\n",
            "2019-04-02T00:52:08.272782: step 10, loss 3.5477, acc 0.05\n",
            "2019-04-02T00:52:10.630184: step 20, loss 3.44087, acc 0.05\n",
            "2019-04-02T00:52:13.041304: step 30, loss 3.459, acc 0.3\n",
            "2019-04-02T00:52:15.358010: step 40, loss 3.72981, acc 0.25\n",
            "2019-04-02T00:52:17.660466: step 50, loss 3.43252, acc 0.1\n",
            "2019-04-02T00:52:20.175329: step 60, loss 3.41778, acc 0.25\n",
            "2019-04-02T00:52:22.427897: step 70, loss 3.32825, acc 0.2\n",
            "2019-04-02T00:52:24.537871: step 80, loss 3.38857, acc 0.15\n",
            "2019-04-02T00:52:26.944389: step 90, loss 3.31821, acc 0.25\n",
            "2019-04-02T00:52:29.127070: step 100, loss 3.05821, acc 0.35\n",
            "\n",
            "Evaluation:\n",
            "2019-04-02T00:52:39.608925: step 100, loss 3.38168, acc 0.37753\n",
            "===========logger===========\n",
            "/content/runs/1554166323/logs/predictions.txt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-102-10a279eea53c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0;31m# Model checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-100-5606d90b9c5c>\u001b[0m in \u001b[0;36mlogging_eval\u001b[0;34m(self, step, loss, accuracy, predictions)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mtarget_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"drive/My Drive/7374Assignment4/SemEval2010_task8_all_data/resource/target.txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mprocess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuprediction_pathbprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"perl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperl_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_path\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mstr_parse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'suprediction_pathbprocess' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "aSpcStj4Ra3W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}